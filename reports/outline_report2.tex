% To show the structur of the outline, i thought that we could use simple latex commands. For instance, this a comment, since it starts with %, and will not appear in the finished report. 




\section{Problem and Approaches}
The task was to determine patterns in the choice of courses computer science students of 
University of Helsinki take. We were given a data set containing the courses and their 
metadata taken by students over a number of years. The methods of representation we used in a previous 
attempt, frequent itemsets, was not optimal. It did solve our problem, but it also represented 
mundane and redundant information. Therefore, we were introduced to the methods
of maximum and closed itemsets during class.
\newline
We employed a new approach to solve the problem this week. We extended our own 
implementation for itemset generation to create maximal and closed frequent itemset. Furthermore we 
concentrated on the second part of the association rule 
mining stategy, the actual rule generation. In our implementation, we also considered different 
measurement methods to determine the value of a rule and an itemset.
\newline
The eclat software was still used, but mainly for the purpose of verifying the outcome of our own 
implementation.

\section{Data}

In addition to the course information from last week, we now had more
specific data about the courses. This metadata consisted of the time period the course was held,
the term, its level and compulsority. Furthermore, it contained information about subprograms,
which it might belong to.

This missing information caused insufficiencies in the interpretation of last weeks results, 
therefore we were now able to analyze the data more intensly.
However, integrating this information caused problems as well, which are further discussed in 
the next section.

\section{Transformation and Command line arguments}


Both the meta data in course\_details.txt and actual FID data in course\_num.txt were transformed.
Firstly, only courses with meta data information were taken into account and FID's not present in
course\_details were omitted when reading course\_num. Secondly all the courses were grouped 
to single entity by FID. 

Each so acquired course instance then had following attributes:
\begin{enumerate}
\item FID - fid
\item NAME - course name, lower case and slugified
\item YEAR - sequence of years the course has been taught, i.e. [1999, 2000, 2004]
\item SUBPROGRAM - subprogram of the course
\item COMPULSORY - P:yes V:no ?:not known
\end{enumerate}

The code and semester information were omitted because they were thought to be non-relevant.
After the data was transformed it was easy to only take into account the courses that are
compulsory, taught on certain year interval and so on. We also added some command line tools to restrict the courses. 
\newline

For example:\newline
%\begin{lstlisting}[For example:] 
$>$ python prob2.py t=0.4 c=0.2 year=2006-2011 compulsory=V strip=2
%\end{lstlisting}
\newline

would only look at the non-compulsory courses that were taught in years from 2006 to 2011 and 
after it would strip of all the transactions with 2 or less items.  In this case minimum support would be
0.4 and minimum confidence 0.2. The reason to apply these kind of restrictions is to look only 
subset of courses which might give more interesting results.
\newline


Restrictions can still be quite tricky if not handled correctly. We had a discussion of what was the most appropriate way to  
restrist course years. If we
did restrict courses to only those which have been taught in time interval $t$ we can't be sure with this
data if course $c$ - which has been taught both in and outside $t$ - is particular transaction was inside $t$ or not. 

We had both forward and against opinions
in group about the matter. Against opinions main point was that it would not make any good assumptions 
and the noise ($c$ iterations outside $t$) in transactions would only increase. Forward opinion thought that
we could probably make some sound assumptions if we also cutted transactions with low item count out.

As mentioned above, we removed courses without metadata from our dataset. However, we had considered different ways of handling those courses.
Each of the three possibility had some disadvantages, which would affect our results and interpretation.

A first possibility was to leave those course with unknown values for the metadata. However, this would have led to 
futher complications, as soon as the metadata was used to limit the considered data to certain courses. 

A second possibility was to just skip those course and exclude them from the data. Yet, this would affect
statistical data such as the average amount of course taken by a student, but also the frequency of the other courses.
Furthermore, if a transaction only consisted of the courses with unknown metadata, the whole transaction would be missing.


A third possiblity was to remove the whole transaction, if one of its courses was without metadata. 
This would not have affected basic statistics, such as an average number of courses assuming that courses without metadata were distributed evenly, which we could not guarantee.
Nonetheless, as we would minimize our dataset of transactions anyway, we might as well remove those.
\newline

After some discussions, we decided on the secound option, removing the courses form our dataset. The disadvantages seemed to be the easiest to handle, so it was the least of the three evils. 








